\section{Some lessons}\label{sec:lessons}

We want to finish this report by gathering some of the conclusions we
reached after the experience of formalizing the basics of forcing in a
proof assistant.

\subsection{Aims of a formalization and planning}

We believe that in every project of formalization of mathematics,
there is a tension between the haste to verify the target results and
the need to obtain a readable, albeit extremely detailed, corpus of
statements and proofs. This tension is mirrored in two different
purposes of formalization: Developing new mathematics from scratch and
producing verified results en route to this, versus verifying and
documenting material that has already been produced on paper.

Our present project clearly belongs to the second category, so we
prioritized trying to obtain formal proofs that mimicked standard
prose (the highlight being the sample proof in
Section~\ref{sec:sample-formal-proof}). We feel that the Isar language
provided with Isabelle has the right balance between elegance and
efficacy. Another crucial aspect to achieve this goal is the level of
detail of the blueprint for the formalization. We must however confess
that we learned many of the subtleties of Isabelle in the making, and
many engineering decisions were also taken before it was clear the
precise way things would develop in the future.

A similar experience, but on an opposite side of the formalization
spectrum happened to the Liquid Tensor Project as described by Scholze
in \cite{LTE2021}. People involved in the formalization simply pushed
their way to reach the summit, formalizing lemma after lemma. They
actually wrote the blueprint for that formalization \emph{afterwards}
it was complete! From time to time, we were also frenziedly trying to
get the results formalized, going beyond what we had planned.

As a result from this, some design choices that seemed reasonable at
first were proved to be inconvenient. For instance, we should had
better used predicates (of type $\tyi\fun\tyi\fun\tyo$) for the
forcing posets' order relations; this is the way they
are presented in the \session{Delta\_System\_Lemma} session. A similar
problem is that we require the forcing poset to be an element of $M$,
so the present infrastructure does not allow class forcing out of the
box. (The latter change seems to be rather straightforward, but the
former does not.)

Nearly the final stage of the project, we decided to go for the minimal
set of definitions and versions of lemmas that were needed to obtain
our target results. For example,
\begin{itemize}
\item
  we only proved the Delta System Lemma for $\aleph_1$-sized families
  (thus limiting us to the plain ccc);
\item we showed preservation of sequences by considering countably
  closed forcings (in fact, we formalized the bare minimum requirement
  of being $(<\omega+1)$-closed); and
\item we proved that appropriate subsets were dense only for Cohen posets
  relevant for the arguments at hand, but could have been done in general.
\end{itemize}
In doing this we went against the tried and
true advice that one should formalize the most general version of the
results available. Another shortcut we took was to
simplify some proofs by appealing to the countability of the ground
model; this is the case of
\isatt{definition{\uscore}of{\uscore}forces} and the result on forcing
values of a function.

\subsection{How to believe in the formalization}
This is a rather tricky question, that was addressed by Pollack in his
\cite{MR1686867}. There is little point to discuss that, after an
assistant has accepted some input successfully, \emph{some
mathematics} has been formally verified. What might not be apparent is
if the claimed theorems are indeed the results that have been
checked. One key aspect of this is the logical foundation of the
assistant (Section~\ref{sec:isabelle-metalogic-meta}). But the weakest
link in the chain is the laying down of definitions building up to the
concepts needed to state the target results. (Another related aspect,
concerning the way results are printed and parsed by assistant versus
there internal meaning, was studied by Wiedijk \cite{zbMATH06319597}.)

We took care of this matter by providing, as an entry point for our
whole development, the theory \theory{Definitions\_Main} in which a
path from some the fundamental concepts from Isabelle/ZF reaching to our main theorems
is expounded. Cross-references to major milestones (which can be
navigated by using Isabelle) are provided there. It
corresponds roughly to less than seven journal pages.

Frequently, we formalized material by directly typing the proof we
new by heart, and in doing so we assumed that some definitions
accommodated some of our preconceptions.
It is significant that in a few such occasions, we were doubly
surprised by the fact that some supposedly trivial lemma would not go
through, because the definitions addressed something different (think
of \emph{restriction} of a function to a set versus that of a
relation), and also that we were able to proved the previous
results. The takeaway is that intuition may drive proofs
even if you are not working on what we believe we are.

A final aspect on this topic concerns automated methods. As it was
mentioned in the Introduction, some proofs can actually be
\emph{obscured} by automation, simply because one has to take
complicated opaque computations “on faith.”

\subsection{Bureaucracy and scale factors}

It is noteworthy that although the “math” of the construction of a
model of $\ZFC+\neg\CH$ was already in place by the end of November
2020, it was only 9 months later that we were able to finish the
formalization of that result. The missing pieces were essentially
bureaucracy. Some of the material filed under this category comprises:
\begin{itemize}
\item calculation of arities of internalized formulas;
\item proving that certain constructions belong to the relevant
  models;
\item (required for the above) showing that particular instances of
  separation and replacement hold in the ground model;
\item etc.
\end{itemize}

Some of those proofs were almost copy-pasted once and again with minor
variants; this would usually be relegated to some function in the
meta-language, but we were unable to do this due to our limitations in
programming Isabelle/ML\footnote{%
  On the other hand, our inability to automate proofs of replacement
  instances paved the way for identifying which were the ones needed for
  forcing! ($\leftarrow$\textbf{put somewhere, don't know if this is the right
    place}).}
and  (an ML style having several quirks and which
is not very well document).

Nevertheless, the experience of the CS members of the team (and
community) is invaluable for the kind of engineering that is needed in
a large project like the present one. For example, it is
(mathematically) misleading when automatic tools (\isatt{simp},
\isatt{auto}, etc) stop working just because of the sheer size of the
goal (v.g., the same statement with 7 variables succeed but with 8
variables doesn't). Scale issues are very easily disregarded in the
abstract; for example, the formula $\forceisa({\cdot}0\in 1{\cdot})$
can be explicitly printed by Isabelle (it spans nearly 20k symbols), but
$\forceisa({\cdot}\neg{\cdot}\neg{\cdot}0\in 1{\cdot}{\cdot}{\cdot})$
can not.

Another example concerns the very definition of $\forceisa$. As a
proof of concept, one of us tried to obtain its definition by
exclusively using formula synthesis, which was supposed to be trivial,
since it is mathematically (as in the case of
Equation~(\ref{eq:sats_big_union_fm})).
But in fact, some early minor mistake
rendered the whole effort useless. We then turned to a more
informed programming discipline, which involved decomposing the
definition in stages, each of which was checked for correctness, and
in that way we were able to reach our objective.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "independence_ch_isabelle"
%%% ispell-local-dictionary: "american"
%%% End: 
